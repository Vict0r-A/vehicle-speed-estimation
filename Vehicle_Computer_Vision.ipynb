{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-Google Colab Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdHviF7Vdo_D",
        "outputId": "61566209-f66c-47a1-ee9d-c04d3a9d5ce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Dec 25 09:47:46 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi \n",
        "\n",
        "\n",
        "#NVIDIA System Management Interface.\n",
        "#Itâ€™s a CLI (Command Line Interface) tool that comes with the NVIDIA driver installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LAGxexV5dFTE"
      },
      "outputs": [],
      "source": [
        "!pip install -q supervision \"ultralytics<=8.3.40\" #quiet mode â€” suppresses most of the installation output\n",
        "#Installs the Ultralytics YOLO package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2zWrKM3seeBR",
        "outputId": "c554f345-cfc5-4760-9395-950515e4dedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSONDict(\"/root/.config/Ultralytics/settings.json\"):\n",
            "{\n",
            "  \"settings_version\": \"0.0.6\",\n",
            "  \"datasets_dir\": \"/content/datasets\",\n",
            "  \"weights_dir\": \"weights\",\n",
            "  \"runs_dir\": \"runs\",\n",
            "  \"uuid\": \"569f3ba64b326db489132663f79cd37279811de477381b83ac131e6cdd129cbb\",\n",
            "  \"sync\": false,\n",
            "  \"api_key\": \"\",\n",
            "  \"openai_api_key\": \"\",\n",
            "  \"clearml\": true,\n",
            "  \"comet\": true,\n",
            "  \"dvc\": true,\n",
            "  \"hub\": true,\n",
            "  \"mlflow\": true,\n",
            "  \"neptune\": true,\n",
            "  \"raytune\": true,\n",
            "  \"tensorboard\": true,\n",
            "  \"wandb\": false,\n",
            "  \"vscode_msg\": true\n",
            "}\n",
            "ðŸ’¡ Learn more about Ultralytics Settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n"
          ]
        }
      ],
      "source": [
        "# prevent ultralytics from tracking your activity (telemetry)\n",
        "!yolo settings sync=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "XAozkCe6dl1u",
        "outputId": "e4edc491-ea97-4dbd-d333-5c11ccdc276b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vehicles.mp4 asset download complete. \n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vehicles.mp4'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from supervision.assets import VideoAssets, download_assets\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "download_assets(VideoAssets.VEHICLES)\n",
        "#Supervision Assets (like VideoAssets.VEHICLES) are free demo videos and images\n",
        "#video name is 'vehicles.mp4'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fwwRVpySgGWJ"
      },
      "outputs": [],
      "source": [
        "INPUT_VIDEO = \"vehicles.mp4\"\n",
        "OUTPUT_VIDEO = \"vehicles_output.mp4\"\n",
        "MODEL = \"yolov8m.pt\"\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.5\n",
        "MODEL_RESOLUTION = 1280"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pR_deaB0hu_b"
      },
      "outputs": [],
      "source": [
        "# Perspective transformation setup\n",
        "# Defines a mapping from image pixel coordinates to real-world dimensions\n",
        "\n",
        "# Coordinates of the quadrilateral on the image that represents the road surface\n",
        "# Points are ordered clockwise starting from the top-left\n",
        "box_coordinates = np.array([\n",
        "    [1252, 787],    # top-left corner of the road\n",
        "    [2298, 803],    # top-right corner of the road\n",
        "    [5039, 2159],   # bottom-right corner of the road\n",
        "    [-550, 2159]    # bottom-left corner of the road\n",
        "])\n",
        "\n",
        "# Real-world dimensions of the selected road area (in metres)\n",
        "box_width_metres = 25     # approximate road width\n",
        "box_height_metres = 250   # approximate visible road length\n",
        "\n",
        "# Target rectangle in real-world coordinates\n",
        "# This represents the same road area but flattened into a metric space\n",
        "box = np.array([\n",
        "    [0, 0],\n",
        "    [box_width_metres - 1, 0],\n",
        "    [box_width_metres - 1, box_height_metres - 1],\n",
        "    [0, box_height_metres - 1],\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "2WYd-1XFjUFO"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Open the input video for visual inspection\n",
        "cap = cv2.VideoCapture(INPUT_VIDEO)\n",
        "\n",
        "# Loop through each frame to preview the polygon placement\n",
        "while cap.isOpened():\n",
        "\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    # Create a copy of the frame to avoid modifying the original\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # Draw the polygon used as the region of interest\n",
        "    annotated_frame = sv.draw_polygon(\n",
        "        scene=annotated_frame,\n",
        "        polygon=box_coordinates,\n",
        "        color=sv.Color.RED,\n",
        "        thickness=4\n",
        "    )\n",
        "\n",
        "    # Display the frame in Google Colab to verify polygon alignment\n",
        "    # Uncomment the line below when running in Colab\n",
        "    # cv2_imshow(annotated_frame)\n",
        "\n",
        "    # Allow early exit if running in an environment with keyboard input\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture once previewing is complete\n",
        "cap.release()\n",
        "\n",
        "# Window cleanup is not required in Google Colab\n",
        "# cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Qt4Xfbi51YCX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "homography = cv2.getPerspectiveTransform(\n",
        "    box_coordinates .astype(np.float32),\n",
        "    box.astype(np.float32)\n",
        ")\n",
        "\n",
        "#converts detection points from camera pixels into real-world coordinates using the perspective transform\n",
        "def transform_points(points):\n",
        "    if points.size == 0:\n",
        "        return points\n",
        "    pts = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "    warped = cv2.perspectiveTransform(pts, homography)\n",
        "    return warped.reshape(-1, 2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NRc6OKoK18Vt"
      },
      "outputs": [],
      "source": [
        "# Load the YOLOv8 model using the specified weights\n",
        "model = YOLO(MODEL)\n",
        "\n",
        "# Read basic metadata from the input video (FPS, resolution, frame count)\n",
        "video_info = sv.VideoInfo.from_video_path(INPUT_VIDEO)\n",
        "\n",
        "# Generator that yields frames sequentially from the input video\n",
        "frame_generator = sv.get_video_frames_generator(INPUT_VIDEO)\n",
        "\n",
        "# Initialise ByteTrack for multi-object tracking\n",
        "# The frame rate is required to maintain stable track IDs over time\n",
        "byte_track = sv.ByteTrack(\n",
        "    frame_rate=video_info.fps,\n",
        "    track_activation_threshold=CONFIDENCE_THRESHOLD\n",
        ")\n",
        "\n",
        "# Calculate annotation thickness based on video resolution\n",
        "# This keeps drawings readable across different video sizes\n",
        "thickness = sv.calculate_optimal_line_thickness(\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "\n",
        "# Calculate an appropriate text scale for labels based on resolution\n",
        "text_scale = sv.calculate_optimal_text_scale(\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "\n",
        "# Annotator for drawing bounding boxes around detected vehicles\n",
        "bbox_annot = sv.BoxAnnotator(thickness=thickness)\n",
        "\n",
        "# Annotator for displaying tracking IDs and speed labels\n",
        "label_annot = sv.LabelAnnotator(\n",
        "    text_scale=text_scale,\n",
        "    text_thickness=thickness,\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "\n",
        "# Annotator for drawing recent movement traces behind each vehicle\n",
        "# Trace length is set to approximately two seconds\n",
        "trace_annot = sv.TraceAnnotator(\n",
        "    thickness=thickness,\n",
        "    trace_length=video_info.fps * 2,\n",
        "    position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "\n",
        "# Define a polygonal region of interest used to filter detections\n",
        "# Only vehicles inside this area are tracked, counted, and analysed\n",
        "polygon_zone = sv.PolygonZone(\n",
        "    polygon=box_coordinates\n",
        ")\n",
        "\n",
        "# Store recent real-world positions for each tracked vehicle\n",
        "# The deque length corresponds to roughly one second of motion history\n",
        "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WeiZSbJd2_KA"
      },
      "outputs": [],
      "source": [
        "VEHICLE_CLASSES = [2, 3, 5, 7]\n",
        "# car, motorcycle, bus, truck\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yyXvkNok3ALl"
      },
      "outputs": [],
      "source": [
        "# Define the video codec for MP4 output\n",
        "# 'mp4v' is widely supported and works reliably in most environments\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "\n",
        "# Create a VideoWriter to save the processed frames to an output file\n",
        "# The output video will match the input video's frame rate and resolution\n",
        "video_writer = cv2.VideoWriter(\n",
        "    OUTPUT_VIDEO,\n",
        "    fourcc,\n",
        "    video_info.fps,\n",
        "    video_info.resolution_wh\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yW_EcGwn3AG5"
      },
      "outputs": [],
      "source": [
        "# Stores unique tracking IDs so each vehicle is counted only once\n",
        "vehicle_count = set()\n",
        "\n",
        "def calculate_speed(track_id, world_point):\n",
        "    \"\"\"\n",
        "    Estimate the average speed of a tracked vehicle in km/h using\n",
        "    real-world coordinates derived from the perspective transform.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve the stored trajectory (in metres) for this vehicle\n",
        "    coords = coordinates[track_id]\n",
        "\n",
        "    # Append the current real-world position\n",
        "    coords.append(world_point)\n",
        "\n",
        "    # Speed cannot be calculated until at least two positions are available\n",
        "    if len(coords) < 2:\n",
        "        return None\n",
        "\n",
        "    # Calculate the distance travelled between the first and latest points\n",
        "    distance_m = np.linalg.norm(coords[-1] - coords[0])\n",
        "\n",
        "    # Calculate the elapsed time in seconds based on frame rate\n",
        "    time_s = len(coords) / video_info.fps\n",
        "\n",
        "    # Convert metres per second to kilometres per hour\n",
        "    speed_mps = distance_m / time_s\n",
        "    speed_kmh = speed_mps * 3.6\n",
        "\n",
        "    return speed_kmh\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWwruw-I3ADy",
        "outputId": "548c14ef-0c39-4fdd-f1f8-f7e41e0c7d05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 538/538 [01:25<00:00,  6.29it/s]\n"
          ]
        }
      ],
      "source": [
        "for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "    # Run YOLOv8 inference on the current frame\n",
        "    results = model(\n",
        "        frame,\n",
        "        imgsz=MODEL_RESOLUTION,\n",
        "        conf=CONFIDENCE_THRESHOLD,\n",
        "        iou=IOU_THRESHOLD,\n",
        "        verbose=False\n",
        "    )[0]\n",
        "\n",
        "    # Convert YOLO output into Supervision detections\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "    # Keep only vehicle-related classes (car, motorcycle, bus, truck)\n",
        "    mask = np.isin(detections.class_id, VEHICLE_CLASSES)\n",
        "    detections = detections[mask]\n",
        "\n",
        "    # Keep detections that fall inside the polygon region of interest\n",
        "    detections = detections[polygon_zone.trigger(detections)]\n",
        "\n",
        "    # Apply ByteTrack to assign consistent tracking IDs\n",
        "    detections = byte_track.update_with_detections(detections)\n",
        "\n",
        "    # If no vehicles are present, write the original frame and continue\n",
        "    if len(detections) == 0:\n",
        "        video_writer.write(frame)\n",
        "        continue\n",
        "\n",
        "    # Extract bottom-centre anchor points for each detection\n",
        "    points = detections.get_anchors_coordinates(\n",
        "        anchor=sv.Position.BOTTOM_CENTER\n",
        "    )\n",
        "\n",
        "    # Transform image coordinates into real-world metres\n",
        "    world_points = transform_points(points)\n",
        "\n",
        "    # Prepare labels containing tracking ID and estimated speed\n",
        "    labels = []\n",
        "    for tracker_id, world_pt in zip(detections.tracker_id, world_points):\n",
        "\n",
        "        speed = calculate_speed(tracker_id, world_pt)\n",
        "\n",
        "        # Count each vehicle once using its unique tracking ID\n",
        "        vehicle_count.add(tracker_id)\n",
        "\n",
        "        if speed is None:\n",
        "            labels.append(f\"ID {tracker_id}\")\n",
        "        else:\n",
        "            labels.append(f\"ID {tracker_id} | {int(speed)} km/h\")\n",
        "\n",
        "    # Create a copy of the frame for drawing annotations\n",
        "    annotated = frame.copy()\n",
        "\n",
        "    # Draw the polygon defining the monitored road area\n",
        "    annotated = sv.draw_polygon(\n",
        "        annotated,\n",
        "        box_coordinates,\n",
        "        color=sv.Color.RED,\n",
        "        thickness=2\n",
        "    )\n",
        "\n",
        "    # Draw bounding boxes, tracking traces, and labels\n",
        "    annotated = bbox_annot.annotate(annotated, detections)\n",
        "    annotated = trace_annot.annotate(annotated, detections)\n",
        "    annotated = label_annot.annotate(annotated, detections, labels)\n",
        "\n",
        "    # Display the total vehicle count at the top centre with a background panel\n",
        "    text = f\"Vehicles counted: {len(vehicle_count)}\"\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "    font_scale = video_info.resolution_wh[0] / 1200\n",
        "    thickness = 2\n",
        "\n",
        "    (text_width, text_height), _ = cv2.getTextSize(\n",
        "        text, font, font_scale, thickness\n",
        "    )\n",
        "\n",
        "    x = (video_info.resolution_wh[0] - text_width) // 2\n",
        "    y = text_height + 20\n",
        "\n",
        "    pad_x = 20\n",
        "    pad_y = 15\n",
        "\n",
        "    overlay = annotated.copy()\n",
        "\n",
        "    cv2.rectangle(\n",
        "        overlay,\n",
        "        (x - pad_x, y - text_height - pad_y),\n",
        "        (x + text_width + pad_x, y + pad_y),\n",
        "        (40, 40, 40),\n",
        "        -1\n",
        "    )\n",
        "\n",
        "    cv2.addWeighted(overlay, 0.6, annotated, 0.4, 0, annotated)\n",
        "\n",
        "    cv2.putText(\n",
        "        annotated,\n",
        "        text,\n",
        "        (x, y),\n",
        "        font,\n",
        "        font_scale,\n",
        "        (255, 255, 255),\n",
        "        thickness,\n",
        "        cv2.LINE_AA\n",
        "    )\n",
        "\n",
        "    # Write the fully annotated frame to the output video\n",
        "    video_writer.write(annotated)\n",
        "\n",
        "# Finalise and close the output video file\n",
        "video_writer.release()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Q2I_lcCFeI1k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
